\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}\bfseries  Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}\bfseries  Prediction}{3}{chapter.2}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}\slshape   Time Series}{3}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Schema of a process.\relax }}{3}{figure.caption.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}\slshape   Prediction}{3}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}\slshape   Prediction Horizon}{4}{section.2.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}\bfseries  Artificial Neural Networks}{5}{chapter.3}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}\slshape   Artificial Neuron}{5}{section.3.1}}
\citation{universal-approx-theorem}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}\slshape   Feedforward Neural Networks}{6}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces On the left, MLP consisting of input layer with two units, two hidden layers with four and three units respectively, and output layer with two untis. Schematic diagram of the MLP's layers on the right.\relax }}{7}{figure.caption.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}\rmfamily   Backpropagation}{7}{subsection.3.2.1}}
\citation{phoneme-recognition-tdnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}\rmfamily   Time-Delay Neural Networks}{9}{subsection.3.2.2}}
\citation{rumelhart-hinton-williams}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}\slshape   Recurrent Neural Networks}{10}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}\rmfamily   Elman's Simple Recurrent Network}{10}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces On the left, Elman's recurrent network with input, hidden, context, and output layer, each containing two units. On the right, schema of layers in Elman network. Dotted link signifies copying activity of source units to target units.\relax }}{11}{figure.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}\rmfamily   Backpropagation Through Time}{11}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Elman's recurrent network unfolded in time. \relax }}{12}{figure.caption.11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{unfolding}{{3.3}{12}{Elman's recurrent network unfolded in time. \relax }{figure.caption.11}{}}
\newlabel{fig:bptt}{{3.3}{12}{Elman's recurrent network unfolded in time. \relax }{figure.caption.11}{}}
\citation{Williams90anefficient}
\citation{williams-zipser}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}\rmfamily   Truncated Backpropagation Through Time}{13}{subsection.3.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}\rmfamily   Real-Time Recurrent Learning}{14}{subsection.3.3.4}}
\citation{minds-jacobs}
\citation{Hochreiter01gradientflow}
\citation{cw-rnn}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}\slshape   Clockwork Recurrent Network}{15}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Clockwork-RNN. \relax }}{16}{figure.caption.12}}
\newlabel{cwrnn}{{3.4}{16}{Clockwork-RNN. \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}\bfseries  Implementation}{17}{chapter.4}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}\slshape   Neural Prediction Tool}{17}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}\rmfamily   Data Sets}{17}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}\rmfamily   Neural Networks}{17}{subsection.4.1.2}}
\citation{dstat}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}\rmfamily   Learning Algorithms}{18}{subsection.4.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}\slshape   Network Monitor}{18}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Sample output from Dstat.\relax }}{18}{figure.caption.13}}
\citation{bepuphysics}
\citation{bepu-robotarm-demo}
\citation{internal-model-control}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}\slshape   Robotic Arm Simulator}{19}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Robotic arm in BEPUPhysics 3D physics simulator.\relax }}{19}{figure.caption.14}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}\bfseries  Experiments}{20}{chapter.5}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}\slshape   Testing Scenarios}{20}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}\rmfamily   Goniometric function}{21}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}\rmfamily   Network Usage}{21}{subsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}\rmfamily   Robotic Arm}{22}{subsection.5.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}\slshape   Tested Methods}{22}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}\rmfamily   Time Delay Neural Network}{22}{subsection.5.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Learning and momentum rates of TDNN in the experiments.\relax }}{23}{table.caption.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}\rmfamily   SRN trained by TBPTT}{23}{subsection.5.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Learning and momentum rates of SRN trained by BPTT in the experiments.\relax }}{23}{table.caption.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}\rmfamily   SRN trained by RTRL}{23}{subsection.5.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}\rmfamily   CW-RNN trained by TBPTT}{23}{subsection.5.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Learning and momentum rates of CW-RNN trained by BPTT in the experiments.\relax }}{24}{table.caption.17}}
\@input{results/tdnn_gon_error.aux}
\@input{results/tbptt_gon_error.aux}
\@input{results/cw_gon_error.aux}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}\bfseries  Results}{25}{chapter.6}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}\slshape   Goniometric Function Results}{25}{section.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}\rmfamily   Role of TDNN's Sliding Window Size}{25}{subsection.6.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Total error of predicting goniometric function with TDNN and various sliding window sizes.\relax }}{25}{figure.caption.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}\rmfamily   Role of TBPTT Unfolding Depth}{25}{subsection.6.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Total error of predicting goniometric function with SRN trained by TBPTT with various unfolding depth.\relax }}{26}{figure.caption.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Total error of predicting goniometric function with CW-RNN trained by TBPTT with various unfolding depth.\relax }}{26}{figure.caption.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}\rmfamily   Computation Time Tradeoff}{26}{subsection.6.1.3}}
\@input{results/gon_tradeoff.aux}
\@input{results/tdnn_net_error.aux}
\@input{results/tbptt_net_error.aux}
\@input{results/cw_net_error.aux}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Tradeoff between time and error in goniometric function scenario.\relax }}{27}{figure.caption.20}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}\slshape   Network Usage Results}{27}{section.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Total error of predicting network traffic with TDNN and various sliding window sizes.\relax }}{28}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Total error of predicting network traffic with SRN trained by TBPTT with various unfolding depth.\relax }}{28}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Total error of predicting network traffic with CW-RNN trained by TBPTT with various unfolding depth.\relax }}{28}{figure.caption.21}}
\@input{results/tdnn_man_error.aux}
\@input{results/tbptt_man_error.aux}
\@input{results/cw_man_error.aux}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}\slshape   Robotic Arm Results}{29}{section.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}\rmfamily   Role of TDNN's Sliding Window Size}{29}{subsection.6.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Total error of predicting manipulator's claw position with TDNN and various sliding window sizes.\relax }}{29}{figure.caption.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}\rmfamily   Role of TBPTT Unfolding Depth}{29}{subsection.6.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Total error of predicting manipulator's claw position with SRN trained by TBPTT with various unfolding depth.\relax }}{30}{figure.caption.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Total error of predicting manipulator's claw position with CW-RNN trained by TBPTT with various unfolding depth.\relax }}{30}{figure.caption.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}\rmfamily   Computation Time Tradeoff}{30}{subsection.6.3.3}}
\@input{results/man_tradeoff.aux}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Tradeoff between time and error in robotic arm scenario.\relax }}{31}{figure.caption.24}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}\bfseries  Conclusion}{32}{chapter.7}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\bibstyle{plain}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}\bfseries  Appendix}{33}{appendix.A}}
\@writefile{lof}{\addvspace {4\p@ }}
\@writefile{lot}{\addvspace {4\p@ }}
\bibcite{bepuphysics}{1}
\bibcite{bepu-robotarm-demo}{2}
\bibcite{dstat}{3}
\bibcite{universal-approx-theorem}{4}
\bibcite{Hochreiter01gradientflow}{5}
\bibcite{minds-jacobs}{6}
\bibcite{internal-model-control}{7}
\bibcite{cw-rnn}{8}
\bibcite{rumelhart-hinton-williams}{9}
\bibcite{phoneme-recognition-tdnn}{10}
\bibcite{Williams90anefficient}{11}
\bibcite{williams-zipser}{12}
\@ifundefined{unitindent}{\newdimen\unitindent\let \@indentset\relax }{}
\global \unitindent=30.0pt\relax\@indentset\relax
